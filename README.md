# ğŸ“‰ MACHINE LEARNING: LIDIANDO CON DATOS DE MUCHAS DIMENSIONES

[![Python](https://img.shields.io/badge/Python-3670A0?style=flat&logo=python&logoColor=ffdd54)](https://www.python.org/)Â Â 
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-F7931E?style=flat&logo=scikit-learn&logoColor=white)](https://scikit-learn.org/)Â Â 
[![NumPy](https://img.shields.io/badge/NumPy-013243?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)Â Â 
[![Pandas](https://img.shields.io/badge/Pandas-150458?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)

Este proyecto aborda la problemÃ¡tica de la **Alta Dimensionalidad** en Machine Learning, donde un dataset posee un gran nÃºmero de *features* (columnas). El objetivo es aplicar y comprender la tÃ©cnica de **ReducciÃ³n de Dimensionalidad** para transformar los datos a un espacio dimensional menor, manteniendo las propiedades originales esenciales, lo que se conoce como **DimensiÃ³n IntrÃ­nseca**.

---

## ğŸ§  Contenido del Proyecto

### 1ï¸âƒ£ El Problema de la Alta Dimensionalidad
- **Contexto:** Se utiliza un dataset ficticio llamado `aluracare.csv` (adaptado de un dataset real de cÃ¡ncer de mama de Kaggle) que contiene muchas caracterÃ­sticas o dimensiones.
- **La MaldiciÃ³n de la Dimensionalidad:** Cuando el nÃºmero de *features* es muy grande, los modelos de Machine Learning pueden volverse ineficientes, lentos e incluso propensos al sobreajuste (*overfitting*). Reducir la dimensionalidad es clave para mejorar el rendimiento.

### 2ï¸âƒ£ Estrategias de ReducciÃ³n de Dimensionalidad
El *notebook* explora diversas tÃ©cnicas para lidiar con el alto nÃºmero de dimensiones:

#### A. SelecciÃ³n de Features (Filtrado)
- Se utiliza el clasificador **Random Forest** para calcular la **Importancia de las Variables**.
- El objetivo es **filtrar** y quedarnos solo con las variables que tienen un mayor poder predictivo sobre la etiqueta de salida.

#### B. ExtracciÃ³n de Features (PCA)
- **AnÃ¡lisis de Componentes Principales (PCA):** Es la tÃ©cnica principal de extracciÃ³n.
- **MecÃ¡nica:** PCA identifica los ejes (componentes principales) que capturan la mayor varianza de los datos. Permite proyectar el dataset original en un nuevo subespacio con menos dimensiones.
- **Ventaja:** Permite pasar, por ejemplo, de 10 *features* a solo 2 o 3, simplificando el modelo sin perder informaciÃ³n crucial.

### 3ï¸âƒ£ AplicaciÃ³n PrÃ¡ctica con SKLearn
- Se utilizan las utilidades de **Scikit-learn** para implementar tanto la SelecciÃ³n de Features como las transformaciones de **PCA**.
- La reducciÃ³n de dimensionalidad es un paso fundamental en el **Preprocesamiento de Datos** para garantizar que el modelo de Machine Learning final sea eficiente, interpretable y generalice mejor.

---

## ğŸ› ï¸ LibrerÃ­as Utilizadas

| LibrerÃ­aÂ  Â  Â  Â | Uso principalÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â |
|----------------|---------------------------------------------|
| **Scikit-learn**| ImplementaciÃ³n de PCA y modelos para determinar la Importancia de Variables|
| **Pandas**Â  Â  Â | Carga y manipulaciÃ³n del dataset (`aluracare.csv`)|
| **NumPy**Â  Â  Â  | Operaciones numÃ©ricas subyacentes (inferido) |

---

## ğŸ¯ Objetivo del Proyecto
Capacitar al usuario para identificar y resolver el problema de la **Alta Dimensionalidad** en Data Science. El proyecto enseÃ±a las diferencias y aplicaciones prÃ¡cticas de la **SelecciÃ³n de Features** y la **ExtracciÃ³n de Features (PCA)**, permitiendo construir modelos de Machine Learning mÃ¡s robustos, rÃ¡pidos y precisos.

---

## ğŸ“ˆ Resultados Clave
- Se demuestra cÃ³mo el **AnÃ¡lisis de Componentes Principales (PCA)** transforma un dataset complejo a una representaciÃ³n bidimensional o tridimensional mÃ¡s simple.
- Se identifica la **DimensiÃ³n IntrÃ­nseca** del dataset, es decir, el nÃºmero Ã³ptimo de *features* necesarias para representar la informaciÃ³n de manera eficiente.
- El proceso resulta en un conjunto de datos preprocesado y optimizado, listo para ser utilizado en cualquier modelo predictivo con menor riesgo de sobreajuste.

